
## Files:
- Annotation guidance (PDF)

- 10K annotations: CSV

## Number of evaluators in total
3 annotators per document per Attitude, concern, or emotion category minimum - 3 or more annotators focused exclusively on the Moral Framing labels, 3 or more on the Attitudes, etc. Each group worked on an identical dataset but were presented only with the labels of the category they were assigned, then all label data was combined in our final eval dataset. There were approximately 15 annotators in total. In some cases, more than 3 annotators labeled a given tweet for a given label category.

## Information about evaluators per tweet
All annotators were presented with all labels (there was no grouping). 1

## Exact questions given to evaluators to determine the emotions
In 1B, annotators were given a training document (labeling guidelines) and participated in a series of training and check-in meetings to ensure that they understood the tasks (and had an opportunity to ask clarifying questions) and were using a consistent labeling approach.

## Annotations were binary, multilabeling was possible
Annotators were instructed to label whether there was a given attitude, concern, or emotion in each tweet. Multiple and overlapping annotations (such as multiple emotions or multiple attitudes with a given tweet) were possible. Each time a given annotator placed 1 or more label(s) in a document, we considered that to be equivalent to a document-level binary 1 (yes, present) with respect to that label for that annotator, and lack of a label (or application of a categorically negative 'none of these' label) was treated as document-level binary 0 (for that category/document/annotator combination).
